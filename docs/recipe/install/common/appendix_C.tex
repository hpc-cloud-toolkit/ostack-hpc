\subsection{Verifying Installation and Troubleshooting}

This section provides a guide to troubleshoot the installation. The information here is based on a sample instantiation of "HPC in an OpenStack Cloud" as described in this document. This document assumes that OpenStack credentials are stored at /root/keystonerc\_admin.\\
\\
\textbf{Verify the HPC instances}\\
    First step of troubleshooting is to verify your HPC instances. Check instances via nova
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[controller]# source ~/keystonerc\_admin
[controller]# nova list
\end{lstlisting}
Output of above command shall list cloud compute nodes as per the configuration mentioned in the cofiguration file. Status of each node will tell you if nodes are active (running) or not. you should see IP address for each node on "Networks" column of the output. Below is is the output of nova command for a successfull instantiation. \\
+--------------------------------------+-------+--------+------------+-------------+---------------------------+\\
| ID                                   | Name  | Status | Task State | Power State | Networks                  |\\
+--------------------------------------+-------+--------+------------+-------------+---------------------------+\\
| cf19ef93-b8ca-4429-843e-1d4ece0d5719 | cc1   | ACTIVE | -          | Running     | sharednet1=192.168.46.101 |\\
| 541cb096-f40c-4658-8bdc-beff15e9f13c | cc2   | ACTIVE | -          | Running     | sharednet1=192.168.46.102 |\\
| 1abbd309-0cc2-4f47-985e-140e2dd51ee6 | sms11 | ACTIVE | -          | Running     | sharednet1=192.168.46.103 |\\
+--------------------------------------+-------+--------+------------+-------------+---------------------------+\\
\\
In case you don't see status as "ACTIVE" for your instances, you might want to check status of nodes via ironic. 
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[controller]# source ~/keystonerc\_admin
[controller]# ironic node-list
\end{lstlisting}
This shall return a output described below (UUID will be different for each instances).\\
+--------------------------------------+-------+--------------------------------------+-------------+--------------------+-------------+\\
| UUID                                 | Name  | Instance UUID                        | Power State | Provisioning State | Maintenance |\\
+--------------------------------------+-------+--------------------------------------+-------------+--------------------+-------------+\\
| d6c56ee2-3714-455e-a696-ba809a9763b5 | sms11 | 1abbd309-0cc2-4f47-985e-140e2dd51ee6 | power on    | active             | False       |\\
| 23cdd2d8-3f63-463a-8644-b21040d63798 | cc1   | cf19ef93-b8ca-4429-843e-1d4ece0d5719 | power on    | active             | False       |\\
| 41cb60a0-7239-4378-93b6-1dfc8f377fdc | cc2   | 541cb096-f40c-4658-8bdc-beff15e9f13c | power on    | active             | False       |\\
+--------------------------------------+-------+--------------------------------------+-------------+--------------------+-------------+\\

Common problem observed during test setup is that ironic node is stuck at "wait call-back" state for long time before moving to ERROR state. Possible problem for that is your security setting at controller node. \\
- Check for any firewall rules which might block your PXE packets from SMS node to controller node and vise versa. \\
- Check if nodes are getting IP address. You can watch the network traffic via tcpdump utility.
- Check if deploy images are build correctly. Verify if glance has entry for deploy images via "glance image-list". Also verify deply image at location "/opt/ohpc/admin/images/cloud/", look for "icloud-hpc-deploy*.kernel" and "icloud-hpc-deploy*.initramfs". If you don't see these images than refere to the section "Preparing ironic deploy images" of this document. \\
\\
\textbf{Verify SSH connections}\\
Next step is to verify a ssh connection from the controller node to the SMS node and then from the SMS node to the compute nodes. First get the IPAddress of the SMS node via "nova list" command and use that IP address to verify the ssh connection with command "ssh <sms\_ip> hostname -I". You should get IP address of the SMS node on output. 
 
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[controller]# ssh 192.168.46.103 hostname -I
192.168.46.103
[controller]# ssh 192.168.46.103 hostname 
sms11
\end{lstlisting}
If SSH from the controller\_node to the SMS node does not work, then you need to verify your instances and check any possible errors by observing output of "nova show <node name>" or <ironic node-show <node-name>
Similar to ssh to SMS node, verify SSH connection from the controller node to the compute nodes
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[controller]# ssh 192.168.46.101 hostname -I
192.168.46.101
[controller]# ssh 192.168.46.101 hostname 
cc1
[controller]# ssh 192.168.46.102 hostname
cc2
\end{lstlisting}
Verify SSH connection from SMS node to compute node
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[controller]# ssh 192.168.46.103 "ssh cc1 hostname -I"
192.168.46.101
[controller]# ssh 192.168.46.103 "ssh cc1 hostname "
cc1
[controller]# ssh 192.168.46.103 "ssh cc2 hostname" 
cc2
\end{lstlisting}
	
